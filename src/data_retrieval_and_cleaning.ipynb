{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6872dab5",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Get facility_code and relative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2d2227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"API.env\")\n",
    "API_KEY = os.getenv(\"OE_API_KEY\")\n",
    "HEADERS = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "\n",
    "def fetch_facilities():\n",
    "    \"\"\"\n",
    "    Retrieve the full list of facilities from OpenElectricity using pagination.\n",
    "    The endpoint returns pages with a 'links.next' URL; we keep following 'next'\n",
    "    until it is None. We also validate the Content-Type to ensure JSON is returned.\n",
    "    Returns:\n",
    "        list[dict]: Raw facility objects as returned by the API across all pages.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openelectricity.org.au/v4/facilities/\"\n",
    "    facilities = []\n",
    "\n",
    "    while url:\n",
    "        # Single page request with a 30s timeout to avoid hanging.\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "\n",
    "        # Defensive: ensure the server actually returned JSON before parsing.\n",
    "        if not resp.headers.get(\"content-type\", \"\").lower().startswith(\"application/json\"):\n",
    "            print(\"⚠️ Not JSON:\", resp.status_code)\n",
    "            print(resp.text[:300])  # print a short snippet to help debugging\n",
    "            break\n",
    "\n",
    "        # Raise an HTTPError if a non-2xx status code occurred (e.g., 401/403/500)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        # Parse JSON body; use empty dict fallback if somehow None\n",
    "        j = resp.json() or {}\n",
    "\n",
    "        # Append the current page's 'data' array to our running list\n",
    "        facilities.extend(j.get(\"data\", []))\n",
    "\n",
    "        # Follow pagination: if 'links.next' exists, continue; else stop loop\n",
    "        url = (j.get(\"links\") or {}).get(\"next\")\n",
    "        if not url:\n",
    "            break\n",
    "\n",
    "    return facilities\n",
    "\n",
    "# Call the fetcher once to retrieve all facilities\n",
    "facilities = fetch_facilities()\n",
    "\n",
    "def facilities_to_df(items):\n",
    "    \"\"\"\n",
    "    Transform the raw facility objects into a tidy pandas DataFrame containing\n",
    "    only the fields we need downstream. If a facility has multiple units, we\n",
    "    read fuel technology from the first unit as a simple representative choice.\n",
    "    Args:\n",
    "        items (list[dict]): Raw facility JSON objects.\n",
    "    Returns:\n",
    "        pandas.DataFrame: Clean table with one row per facility_code.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for f in items:\n",
    "        # If 'units' exists, take the first unit; otherwise fall back to empty dict.\n",
    "        unit = (f.get(\"units\") or [{}])[0]  # first generation unit if available\n",
    "\n",
    "        # Build a flat record; use .get(..., None) style to avoid KeyError.\n",
    "        rows.append({\n",
    "            \"facility_code\": f.get(\"code\"),\n",
    "            \"facility_name\": f.get(\"name\"),\n",
    "            # Some schemas may expose 'region' instead of 'network_region';\n",
    "            # this code prefers 'network_region' to match NEM naming like NSW1.\n",
    "            \"region\": f.get(\"network_region\"),\n",
    "            \"fuel_tech\": unit.get(\"fueltech_id\"),\n",
    "            # Location is nested under 'location' with 'lat'/'lng' keys.\n",
    "            \"lat\": (f.get(\"location\") or {}).get(\"lat\"),\n",
    "            \"lon\": (f.get(\"location\") or {}).get(\"lng\"),\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame and perform basic cleaning:\n",
    "    # - drop rows missing facility_code (we need a stable key)\n",
    "    # - drop duplicate facility_code rows\n",
    "    # - sort by facility_code for deterministic output\n",
    "    df = (pd.DataFrame(rows)\n",
    "            .dropna(subset=[\"facility_code\"])\n",
    "            .drop_duplicates(subset=[\"facility_code\"])\n",
    "            .sort_values(\"facility_code\")\n",
    "            .reset_index(drop=True))\n",
    "    return df\n",
    "\n",
    "# Convert to a clean table and persist to CSV for later tasks\n",
    "df_codes = facilities_to_df(facilities)\n",
    "\n",
    "# Ensure the data directory exists before writing (optional safeguard)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Save as UTF-8 CSV (no index column)\n",
    "df_codes.to_csv(\"data/facilities.csv\", index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bc151b",
   "metadata": {},
   "source": [
    "1) Retrieve per-facility power and CO2 emissions (5m) for one week.\n",
    "2) (Optional) Retrieve market price and demand (5m) by region.\n",
    "3) Integrate timeseries into a single CSV for later MQTT streaming / dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb2d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, time, requests\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9065b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[plan] FULL RUN facilities=579\n",
      "[estimate] facility requests ≈ 1680\n",
      "[batch 0] 0BCWF.. (+29 more)\n",
      "[batch 1] 0RIVERINASF.. (+29 more)\n",
      "[batch 2] APS.. (+29 more)\n",
      "[batch 3] BIALAWF.. (+29 more)\n",
      "[batch 4] BUTLERSG.. (+29 more)\n",
      "[batch 5] COLLIE_BESS2.. (+29 more)\n",
      "[batch 6] DEIBDL.. (+29 more)\n",
      "[batch 7] GIRGSF.. (+29 more)\n",
      "[batch 8] HAYMSF.. (+29 more)\n",
      "[batch 9] JUNEESF.. (+29 more)\n",
      "[batch 10] LIMOSF2.. (+29 more)\n",
      "[batch 11] METZSF.. (+29 more)\n",
      "[batch 12] MUSSELRO.. (+29 more)\n",
      "[batch 13] PHOENIX_KWINANA_WTE.. (+29 more)\n",
      "[batch 14] ROWALLAN.. (+29 more)\n",
      "[batch 15] SNOWY4.. (+29 more)\n",
      "[batch 16] TARONGN.. (+29 more)\n",
      "[batch 17] URANQ.. (+29 more)\n",
      "[batch 18] WILLHOV.. (+29 more)\n",
      "[batch 19] YAMBUK.. (+8 more)\n",
      "[done] facility raw points: 1973434 | elapsed=995.0s\n",
      "[save] data/facility_timeseries_all.csv rows=927922\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Config (edit as needed) --------------------\n",
    "WEEK_START = \"2025-10-12T00:00:00\"   # UTC, without trailing Z\n",
    "WEEK_END   = \"2025-10-18T23:59:59\"\n",
    "INTERVAL   = \"5m\"\n",
    "\n",
    "BATCH_SIZE  = 30      # Number of facilities per request (30–50 is common)\n",
    "CHUNK_HOURS = 2       # Initial chunk length; auto-shrinks on HTTP 416\n",
    "MIN_HOURS   = 1       # Minimum chunk length (hours) for backoff\n",
    "SLEEP_SEC   = 0.05    # Light rate limit between requests; increase on 429\n",
    "\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "OUT_CSV     = f\"{DATA_DIR}/facility_timeseries_all.csv\"\n",
    "FACILITY_CSV= f\"{DATA_DIR}/facilities.csv\"\n",
    "\n",
    "# -------------------- AUTH --------------------\n",
    "# Load from API.env (optional) and environment. Required var: OE_API_KEY\n",
    "load_dotenv(\"API.env\")\n",
    "API_KEY = os.getenv(\"OE_API_KEY\", \"\").strip()\n",
    "\n",
    "# If you prefer to hardcode (NOT recommended for submission), uncomment:\n",
    "# API_KEY = \"oe_xxx_your_token_here\"\n",
    "\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"Missing OE_API_KEY. Set it in environment or API.env.\")\n",
    "\n",
    "HEADERS = {\"Authorization\": f\"Bearer {API_KEY}\", \"Accept\": \"application/json\"}\n",
    "\n",
    "# Endpoints\n",
    "API_BASE_FAC = \"https://api.openelectricity.org.au/v4/data/facilities/NEM\"\n",
    "API_FAC_LIST = \"https://api.openelectricity.org.au/v4/facilities/\"\n",
    "API_BASE_MKT = \"https://api.openelectricity.org.au/v4/market/network/NEM\"\n",
    "\n",
    "# -------------------- HTTP helpers --------------------\n",
    "def http_get(url, params, headers=HEADERS, timeout=30, retry=2):\n",
    "    \"\"\"GET with small retry backoff for transient network issues.\"\"\"\n",
    "    for k in range(retry + 1):\n",
    "        try:\n",
    "            return requests.get(url, headers=headers, params=params, timeout=timeout)\n",
    "        except requests.RequestException:\n",
    "            if k < retry:\n",
    "                time.sleep(0.8 * (k + 1))\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "# -------------------- Facilities master download --------------------\n",
    "def ensure_facilities_csv(path=FACILITY_CSV):\n",
    "    \"\"\"Download facilities master list once and cache as CSV under data/.\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        return pd.read_csv(path)\n",
    "\n",
    "    facilities, url = [], API_FAC_LIST\n",
    "    while url:\n",
    "        r = http_get(url, params={})\n",
    "        r.raise_for_status()\n",
    "        if \"application/json\" not in r.headers.get(\"content-type\",\"\").lower():\n",
    "            raise RuntimeError(f\"Unexpected content-type from {url}\")\n",
    "        j = r.json() or {}\n",
    "        facilities.extend(j.get(\"data\", []))\n",
    "        url = (j.get(\"links\") or {}).get(\"next\")\n",
    "\n",
    "    rows = []\n",
    "    for f in facilities:\n",
    "        rows.append({\n",
    "            \"facility_code\": f.get(\"code\"),\n",
    "            \"facility_name\": f.get(\"name\"),\n",
    "            \"region\": f.get(\"region\") or f.get(\"network_region\"),\n",
    "            \"fuel_tech\": f.get(\"fuel_tech\") or f.get(\"fuel_tech_desc\"),\n",
    "        })\n",
    "\n",
    "    df = (pd.DataFrame(rows)\n",
    "            .dropna(subset=[\"facility_code\"])\n",
    "            .drop_duplicates(subset=[\"facility_code\"])\n",
    "            .sort_values(\"facility_code\").reset_index(drop=True))\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"[init] saved {path}, rows={len(df)}\")\n",
    "    return df\n",
    "\n",
    "# -------------------- Time & batching helpers --------------------\n",
    "def chunk_ranges(start_iso, end_iso, hours):\n",
    "    \"\"\"Yield [start,end] ISO pairs in fixed hour chunks (inclusive).\"\"\"\n",
    "    s = datetime.fromisoformat(start_iso); e = datetime.fromisoformat(end_iso)\n",
    "    cur = s\n",
    "    while cur <= e:\n",
    "        nxt = cur + timedelta(hours=hours) - timedelta(seconds=1)\n",
    "        if nxt > e: nxt = e\n",
    "        yield cur.strftime(\"%Y-%m-%dT%H:%M:%S\"), nxt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        cur = cur + timedelta(hours=hours)\n",
    "\n",
    "def batch_list(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i+batch_size]\n",
    "\n",
    "# -------------------- Parsing helpers (robust to slight schema changes) --------------------\n",
    "def _extract_fac_from_name(series_name: str, metric: str | None):\n",
    "    \"\"\"Try to infer facility_code from 'name' by stripping metric and network prefixes.\"\"\"\n",
    "    if not series_name:\n",
    "        return None\n",
    "    name = str(series_name)\n",
    "    m = (metric or \"\").lower()\n",
    "    low = name.lower()\n",
    "\n",
    "    # remove common network prefixes\n",
    "    for p in (\"nem_\", \"wem_\", \"nem-\", \"wem-\"):\n",
    "        if low.startswith(p):\n",
    "            name = name[len(p):]; low = low[len(p):]; break\n",
    "\n",
    "    # strip metric as prefix or suffix with common separators\n",
    "    for sep in (\"_\", \"-\", \":\", \"/\", \" \"):\n",
    "        if low.startswith(m + sep):\n",
    "            name = name[len(m + sep):]; low = low[len(m + sep):]; break\n",
    "        if low.endswith(sep + m):\n",
    "            name = name[:-(len(sep + m))]; low = low[:-(len(sep + m))]; break\n",
    "\n",
    "    if low in (\"power\", \"emissions\", \"\"):\n",
    "        return None\n",
    "    return name\n",
    "\n",
    "def parse_results_robust(j, default_code=None):\n",
    "    \"\"\"Parse facility timeseries JSON into rows of {facility_code,timestamp,metric,value,unit}.\"\"\"\n",
    "    rows=[]\n",
    "    for ts in (j.get(\"data\") or []):\n",
    "        metric = ts.get(\"metric\")            # \"power\"/\"emissions\"\n",
    "        unit   = ts.get(\"unit\")\n",
    "        results = ts.get(\"results\") or []\n",
    "        for res in results:\n",
    "            if isinstance(res, dict):\n",
    "                series_name = res.get(\"name\") or \"\"\n",
    "                datapoints  = res.get(\"data\") or res.get(\"values\") or []\n",
    "                series_fc   = res.get(\"facility_code\") or res.get(\"facility\")\n",
    "                if not series_fc:\n",
    "                    series_fc = _extract_fac_from_name(series_name, metric)\n",
    "            elif isinstance(res, (list, tuple)):\n",
    "                series_name = \"\"\n",
    "                datapoints  = res\n",
    "                series_fc   = None\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            for dp in datapoints:\n",
    "                if isinstance(dp, dict):\n",
    "                    ts0 = dp.get(\"timestamp\") or dp.get(\"ts\")\n",
    "                    val = dp.get(\"value\") or dp.get(\"v\")\n",
    "                    fc  = dp.get(\"facility_code\") or dp.get(\"facility\") or series_fc or default_code\n",
    "                elif isinstance(dp, (list, tuple)) and len(dp) >= 2:\n",
    "                    ts0, val = dp[0], dp[1]\n",
    "                    fc = series_fc or default_code\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                if fc and str(fc).lower() not in (\"power\",\"emissions\") and ts0 is not None and val is not None:\n",
    "                    rows.append({\n",
    "                        \"facility_code\": fc,\n",
    "                        \"timestamp\": ts0,\n",
    "                        \"metric\": metric,\n",
    "                        \"value\": val,\n",
    "                        \"unit\": unit\n",
    "                    })\n",
    "    return rows\n",
    "\n",
    "def _extract_region_from_name(series_name: str, metric: str | None):\n",
    "    \"\"\"Extract NEM region (NSW1/QLD1/SA1/TAS1/VIC1) by removing metric and prefixes.\"\"\"\n",
    "    if not series_name:\n",
    "        return None\n",
    "    name = str(series_name)\n",
    "    low = name.lower()\n",
    "    m = (metric or \"\").lower()\n",
    "    for p in (\"nem_\", \"nem-\"):\n",
    "        if low.startswith(p):\n",
    "            name = name[len(p):]; low = low[len(p):]; break\n",
    "    for sep in (\"_\", \"-\", \":\", \"/\", \" \"):\n",
    "        if low.startswith(m + sep):\n",
    "            name = name[len(m + sep):]; break\n",
    "        if low.endswith(sep + m):\n",
    "            name = name[:-(len(sep + m))]; break\n",
    "    rg = name.strip()\n",
    "    if rg.lower() in (\"price\",\"demand\",\"\"):\n",
    "        return None\n",
    "    return rg\n",
    "\n",
    "def parse_market_json(j):\n",
    "    \"\"\"Parse /v4/market/network/NEM JSON to rows: {region,timestamp,metric,value,unit}.\"\"\"\n",
    "    rows=[]\n",
    "    for ts in (j or {}).get(\"data\") or []:\n",
    "        metric = ts.get(\"metric\")\n",
    "        unit   = ts.get(\"unit\")\n",
    "        for res in (ts.get(\"results\") or []):\n",
    "            cols   = res.get(\"columns\") or {}\n",
    "            region = cols.get(\"region\") or res.get(\"region\")\n",
    "            name   = res.get(\"name\") or \"\"\n",
    "            if not region:\n",
    "                region = _extract_region_from_name(name, metric)\n",
    "            pts = res.get(\"data\") or res.get(\"values\") or []\n",
    "            for dp in pts:\n",
    "                if isinstance(dp, dict):\n",
    "                    ts0 = dp.get(\"timestamp\") or dp.get(\"ts\")\n",
    "                    val = dp.get(\"value\") or dp.get(\"v\")\n",
    "                elif isinstance(dp, (list,tuple)) and len(dp) >= 2:\n",
    "                    ts0, val = dp[0], dp[1]\n",
    "                else:\n",
    "                    continue\n",
    "                if region and ts0 is not None and val is not None:\n",
    "                    rows.append({\"region\": region, \"timestamp\": ts0,\n",
    "                                 \"metric\": metric, \"value\": val, \"unit\": unit})\n",
    "    return rows\n",
    "\n",
    "# -------------------- API fetchers --------------------\n",
    "def _norm_region(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    s = str(x).strip().upper()\n",
    "    for p in (\"NEM_\", \"NEM-\", \"NEM>\", \"NEM/\"):\n",
    "        if s.startswith(p):\n",
    "            s = s[len(p):]\n",
    "            break\n",
    "    return s\n",
    "\n",
    "def fetch_market_chunk(start_iso, end_iso):\n",
    "    p = {\n",
    "        \"metrics\": [\"price\",\"demand\"],  # If 422 occurs, try \"price,demand\"\n",
    "        \"interval\": INTERVAL,\n",
    "        \"date_start\": start_iso,\n",
    "        \"date_end\":   end_iso,\n",
    "        \"primary_grouping\": \"network_region\",\n",
    "    }\n",
    "    r = http_get(API_BASE_MKT, params=p)\n",
    "    if r.status_code == 416: return \"416\", []\n",
    "    if r.status_code != 200 or \"application/json\" not in r.headers.get(\"content-type\",\"\").lower():\n",
    "        return f\"{r.status_code}\", []\n",
    "    return \"OK\", parse_market_json(r.json())\n",
    "\n",
    "def fetch_facility_chunk(codes, start_iso, end_iso):\n",
    "    p = {\n",
    "        \"metrics\": [\"power\",\"emissions\"],   # Fetch both metrics together\n",
    "        \"facility_code\": list(codes),       # Batch facilities\n",
    "        \"interval\": INTERVAL,\n",
    "        \"date_start\": start_iso, \"date_end\": end_iso,\n",
    "    }\n",
    "    r = http_get(API_BASE_FAC, params=p)\n",
    "    if r.status_code == 416: return \"416\", []\n",
    "    if r.status_code != 200 or \"application/json\" not in r.headers.get(\"content-type\",\"\").lower():\n",
    "        return f\"{r.status_code}\", []\n",
    "    return \"OK\", parse_results_robust(r.json() or {})\n",
    "\n",
    "# -------------------- Main: plan requests --------------------\n",
    "df_fac_master = ensure_facilities_csv(FACILITY_CSV)\n",
    "df_fac_master[\"region\"] = df_fac_master[\"region\"].apply(_norm_region)\n",
    "all_codes = df_fac_master[\"facility_code\"].dropna().astype(str).tolist()\n",
    "\n",
    "print(f\"[plan] FULL RUN facilities={len(all_codes)}\")\n",
    "start = datetime.fromisoformat(WEEK_START)\n",
    "end   = datetime.fromisoformat(WEEK_END)\n",
    "total_hours = int((end - start).total_seconds() // 3600) + 1\n",
    "num_chunks  = ceil(total_hours / CHUNK_HOURS)\n",
    "num_batches = ceil(len(all_codes) / BATCH_SIZE)\n",
    "print(f\"[estimate] facility requests ≈ {num_chunks * num_batches}\")\n",
    "\n",
    "# -------------------- Fetch facility timeseries (single run, no checkpoint) --------------------\n",
    "rows_all = []\n",
    "t0 = time.time()\n",
    "for b_idx, batch in enumerate(batch_list(all_codes, BATCH_SIZE)):\n",
    "    print(f\"[batch {b_idx}] {batch[0]}.. (+{len(batch)-1} more)\")\n",
    "    for s_iso, e_iso in chunk_ranges(WEEK_START, WEEK_END, CHUNK_HOURS):\n",
    "        cur_start, cur_end, cur_hours = s_iso, e_iso, CHUNK_HOURS\n",
    "        while True:\n",
    "            status, rows = fetch_facility_chunk(batch, cur_start, cur_end)\n",
    "            if status == \"OK\":\n",
    "                rows_all.extend(rows)\n",
    "                break\n",
    "            if status == \"416\" and cur_hours > MIN_HOURS:\n",
    "                cur_hours = max(MIN_HOURS, cur_hours/2)\n",
    "                seg_start = datetime.fromisoformat(cur_start)\n",
    "                seg_end   = seg_start + timedelta(hours=cur_hours) - timedelta(seconds=1)\n",
    "                cur_end   = seg_end.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                print(f\"[416] shrink → {cur_hours}h: {cur_start} ~ {cur_end}\")\n",
    "                continue\n",
    "            print(f\"[skip] {cur_start} ~ {cur_end} (status={status})\")\n",
    "            break\n",
    "        time.sleep(SLEEP_SEC)\n",
    "\n",
    "print(f\"[done] facility raw points: {len(rows_all)} | elapsed={time.time()-t0:.1f}s\")\n",
    "\n",
    "# -------------------- Fetch market price/demand (optional) --------------------\n",
    "mkt_rows_all = []\n",
    "for s_iso, e_iso in chunk_ranges(WEEK_START, WEEK_END, CHUNK_HOURS):\n",
    "    status, rows = fetch_market_chunk(s_iso, e_iso)\n",
    "    if status == \"OK\":\n",
    "        mkt_rows_all.extend(rows)\n",
    "    elif status == \"416\":\n",
    "        # Adaptive shrink on HTTP 416\n",
    "        cur_start, cur_hours = s_iso, CHUNK_HOURS\n",
    "        st = status\n",
    "        while cur_hours > MIN_HOURS:\n",
    "            cur_hours = max(MIN_HOURS, cur_hours/2)\n",
    "            seg_end = (datetime.fromisoformat(cur_start)\n",
    "                       + timedelta(hours=cur_hours) - timedelta(seconds=1)).strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "            st, rows2 = fetch_market_chunk(cur_start, seg_end)\n",
    "            if st == \"OK\":\n",
    "                mkt_rows_all.extend(rows2); break\n",
    "        if cur_hours == MIN_HOURS and st != \"OK\":\n",
    "            print(f\"[mkt skip] {s_iso} ~ {e_iso} status={status}\")\n",
    "    else:\n",
    "        print(f\"[mkt skip] {s_iso} ~ {e_iso} status={status}\")\n",
    "\n",
    "df_mkt_wide = pd.DataFrame()\n",
    "if mkt_rows_all:\n",
    "    dfm = pd.DataFrame(mkt_rows_all)\n",
    "    dfm[\"timestamp\"] = pd.to_datetime(dfm[\"timestamp\"], utc=True)\n",
    "    dfm[\"region\"] = dfm[\"region\"].apply(_norm_region)\n",
    "\n",
    "    wstart = pd.Timestamp(WEEK_START + \"Z\"); wend = pd.Timestamp(WEEK_END + \"Z\")\n",
    "    dfm = dfm[(dfm[\"timestamp\"] >= wstart) & (dfm[\"timestamp\"] <= wend)]\n",
    "\n",
    "    df_mkt_wide = (dfm.pivot_table(index=[\"region\",\"timestamp\"],\n",
    "                                   columns=\"metric\", values=\"value\", aggfunc=\"last\")\n",
    "                     .reset_index()\n",
    "                     .rename(columns={\"price\":\"price_aud_per_mwh\",\"demand\":\"demand_mw\"}))\n",
    "\n",
    "# -------------------- Wide table + attributes + (optional) market merge --------------------\n",
    "if rows_all:\n",
    "    df_long = pd.DataFrame(rows_all)\n",
    "    df_long[\"timestamp\"] = pd.to_datetime(df_long[\"timestamp\"], utc=True)\n",
    "\n",
    "    wstart = pd.Timestamp(WEEK_START + \"Z\")\n",
    "    wend   = pd.Timestamp(WEEK_END   + \"Z\")\n",
    "    df_long = df_long[(df_long[\"timestamp\"] >= wstart) & (df_long[\"timestamp\"] <= wend)]\n",
    "\n",
    "    df_wide = (df_long\n",
    "        .pivot_table(index=[\"facility_code\",\"timestamp\"],\n",
    "                     columns=\"metric\", values=\"value\", aggfunc=\"sum\")\n",
    "        .reset_index()\n",
    "        .rename(columns={\"power\":\"power_mw\",\"emissions\":\"co2_kg\"})\n",
    "        .sort_values([\"facility_code\",\"timestamp\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Facility attributes\n",
    "    df_attr = df_fac_master[[\"facility_code\",\"region\",\"fuel_tech\"]].copy()\n",
    "    df_attr[\"region\"] = df_attr[\"region\"].apply(_norm_region)\n",
    "\n",
    "    base = df_wide.merge(df_attr, on=\"facility_code\", how=\"left\")\n",
    "    base[\"timestamp\"] = pd.to_datetime(base[\"timestamp\"], utc=True)\n",
    "    base[\"region\"]    = base[\"region\"].apply(_norm_region)\n",
    "\n",
    "    use_market = (\n",
    "        isinstance(df_mkt_wide, pd.DataFrame)\n",
    "        and not df_mkt_wide.empty\n",
    "        and {\"region\", \"timestamp\"}.issubset(df_mkt_wide.columns)\n",
    "    )\n",
    "\n",
    "    if use_market:\n",
    "        df_mkt_wide = df_mkt_wide.copy()\n",
    "        df_mkt_wide[\"timestamp\"] = pd.to_datetime(df_mkt_wide[\"timestamp\"], utc=True)\n",
    "        df_mkt_wide[\"region\"]    = df_mkt_wide[\"region\"].apply(_norm_region)\n",
    "\n",
    "        df_out = (base.merge(df_mkt_wide, on=[\"region\", \"timestamp\"], how=\"left\")\n",
    "                       .drop_duplicates(subset=[\"facility_code\",\"timestamp\"])\n",
    "                       .sort_values([\"facility_code\",\"timestamp\"])\n",
    "                       .reset_index(drop=True))\n",
    "    else:\n",
    "        print(\"[mkt] empty or missing columns -> skip market merge\")\n",
    "        df_out = (base.drop_duplicates(subset=[\"facility_code\",\"timestamp\"])\n",
    "                       .sort_values([\"facility_code\",\"timestamp\"])\n",
    "                       .reset_index(drop=True))\n",
    "\n",
    "    # Save final integrated CSV\n",
    "    df_out.to_csv(OUT_CSV, index=False)\n",
    "    print(f\"[save] {OUT_CSV} rows={len(df_out)}\")\n",
    "else:\n",
    "    print(\"[warn] no facility timeseries rows collected; nothing saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43487703",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Clean facilitiesl.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb6e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"data/facilities.csv\")\n",
    "\n",
    "# Remove numeric suffix from region (e.g., QLD1 → QLD)\n",
    "df[\"region\"] = df[\"region\"].str.replace(r\"\\d+$\", \"\", regex=True)\n",
    "\n",
    "# Clean facility_name: remove leading/trailing spaces\n",
    "df[\"facility_name\"] = df[\"facility_name\"].str.strip()\n",
    "\n",
    "# Standardize fuel_tech: lowercase and remove extra spaces\n",
    "df[\"fuel_tech\"] = df[\"fuel_tech\"].str.lower().str.strip()\n",
    "\n",
    "# Drop rows with missing latitude or longitude\n",
    "df = df.dropna(subset=[\"lat\", \"lon\"])\n",
    "\n",
    "# Remove duplicated facility_code (keep the first occurrence)\n",
    "df = df.drop_duplicates(subset=\"facility_code\", keep=\"first\")\n",
    "\n",
    "# Export the cleaned dataset\n",
    "df.to_csv(\"data/facilities.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc895dc",
   "metadata": {},
   "source": [
    "Clean facility_timeseries_all.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f690b6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Join complete and cleaned.\n",
      "Rows: 161,731\n",
      "Columns: 11\n",
      "\n",
      "Preview:\n",
      "  facility_code                 timestamp  co2_kg  power_mw region fuel_tech  \\\n",
      "0        AGLHAL 2025-10-12 00:00:00+00:00     0.0       0.0    SA1  gas_ocgt   \n",
      "1        AGLHAL 2025-10-12 00:05:00+00:00     0.0       0.0    SA1  gas_ocgt   \n",
      "2        AGLHAL 2025-10-12 00:10:00+00:00     0.0       0.0    SA1  gas_ocgt   \n",
      "3        AGLHAL 2025-10-12 00:15:00+00:00     0.0       0.0    SA1  gas_ocgt   \n",
      "4        AGLHAL 2025-10-12 00:20:00+00:00     0.0       0.0    SA1  gas_ocgt   \n",
      "\n",
      "   demand_mw  price_aud_per_mwh facility_name       lat         lon  \n",
      "0     340.15             -10.10       Hallett -33.34931  138.752633  \n",
      "1     305.23             -10.10       Hallett -33.34931  138.752633  \n",
      "2     324.03             -10.10       Hallett -33.34931  138.752633  \n",
      "3     294.99              -9.22       Hallett -33.34931  138.752633  \n",
      "4     250.23             -10.47       Hallett -33.34931  138.752633  \n"
     ]
    }
   ],
   "source": [
    "# ===== Join & Clean: facilities × facility_timeseries =====\n",
    "# Keep only matched facility_code records\n",
    "# Remove QA columns, standardize region/fuel_tech, and convert timestamp\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------- File Paths --------------------\n",
    "FACILITIES_CSV = \"data/facilities.csv\"\n",
    "TIMESERIES_CSV = \"data/facility_timeseries_all.csv\"\n",
    "OUTPUT_CSV     = \"data/cleaned_data.csv\"\n",
    "\n",
    "# -------------------- Load Data --------------------\n",
    "fac = pd.read_csv(FACILITIES_CSV)\n",
    "ts  = pd.read_csv(TIMESERIES_CSV, low_memory=False)\n",
    "\n",
    "# -------------------- Basic Cleaning on facilities --------------------\n",
    "# Remove trailing digits from region (QLD1 -> QLD)\n",
    "fac[\"region\"] = fac[\"region\"].str.replace(r\"\\d+$\", \"\", regex=True)\n",
    "\n",
    "# Standardize text fields\n",
    "fac[\"facility_code\"] = fac[\"facility_code\"].astype(str).str.strip()\n",
    "fac[\"facility_name\"] = fac[\"facility_name\"].astype(str).str.strip()\n",
    "fac[\"fuel_tech\"] = fac[\"fuel_tech\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "# -------------------- Inner Join (keep only matched facilities) --------------------\n",
    "merged = ts.merge(fac, on=\"facility_code\", how=\"inner\", suffixes=(\"\", \"_fac\"))\n",
    "\n",
    "# -------------------- Fill Missing Fields --------------------\n",
    "# Fill region and fuel_tech from facilities data\n",
    "merged[\"region\"] = merged[\"region\"].fillna(merged[\"region_fac\"])\n",
    "merged[\"fuel_tech\"] = merged[\"fuel_tech\"].fillna(merged[\"fuel_tech_fac\"])\n",
    "\n",
    "# Remove redundant columns\n",
    "merged = merged.drop(columns=[\"region_fac\", \"fuel_tech_fac\"])\n",
    "\n",
    "# -------------------- Convert Timestamp --------------------\n",
    "merged[\"timestamp\"] = pd.to_datetime(merged[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "merged = merged.dropna(subset=[\"timestamp\"])  # keep only valid timestamps\n",
    "\n",
    "# -------------------- Save Cleaned File --------------------\n",
    "merged.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "# -------------------- Summary --------------------\n",
    "print(\"✅ Join complete and cleaned.\")\n",
    "print(f\"Rows: {len(merged):,}\")\n",
    "print(f\"Columns: {merged.shape[1]}\")\n",
    "print(\"\\nPreview:\")\n",
    "print(merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03312915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaning complete.\n",
      "Rows: 161,731 | Cols: 11\n",
      "Time range: 2025-10-12 00:00:00+00:00 → 2025-10-18 13:55:00+00:00\n",
      "\n",
      "Missing rate (top 10):\n",
      "price_aud_per_mwh    0.018494\n",
      "demand_mw            0.006857\n",
      "facility_code        0.000000\n",
      "timestamp            0.000000\n",
      "co2_kg               0.000000\n",
      "power_mw             0.000000\n",
      "region               0.000000\n",
      "fuel_tech            0.000000\n",
      "facility_name        0.000000\n",
      "lat                  0.000000\n",
      "dtype: float64\n",
      "\n",
      "Region sample counts:\n",
      "region\n",
      "TAS    39759\n",
      "NSW    37878\n",
      "VIC    30304\n",
      "QLD    27274\n",
      "SA     26516\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Fuel tech sample counts:\n",
      "fuel_tech\n",
      "hydro                49244\n",
      "wind                 41651\n",
      "solar_utility        34092\n",
      "gas_ccgt              9470\n",
      "bioenergy_biomass     7561\n",
      "gas_ocgt              5682\n",
      "battery_charging      4616\n",
      "distillate            3788\n",
      "gas_wcmg              3733\n",
      "battery               1894\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Preview:\n",
      "  facility_code                 timestamp  co2_kg  power_mw region fuel_tech  \\\n",
      "0        AGLHAL 2025-10-12 00:00:00+00:00     0.0       0.0     SA  gas_ocgt   \n",
      "1        AGLHAL 2025-10-12 00:05:00+00:00     0.0       0.0     SA  gas_ocgt   \n",
      "2        AGLHAL 2025-10-12 00:10:00+00:00     0.0       0.0     SA  gas_ocgt   \n",
      "3        AGLHAL 2025-10-12 00:15:00+00:00     0.0       0.0     SA  gas_ocgt   \n",
      "4        AGLHAL 2025-10-12 00:20:00+00:00     0.0       0.0     SA  gas_ocgt   \n",
      "\n",
      "   demand_mw  price_aud_per_mwh facility_name       lat         lon  \n",
      "0     340.15             -10.10       Hallett -33.34931  138.752633  \n",
      "1     305.23             -10.10       Hallett -33.34931  138.752633  \n",
      "2     324.03             -10.10       Hallett -33.34931  138.752633  \n",
      "3     294.99              -9.22       Hallett -33.34931  138.752633  \n",
      "4     250.23             -10.47       Hallett -33.34931  138.752633  \n"
     ]
    }
   ],
   "source": [
    "# ===== test2.csv Cleaning Script =====\n",
    "# Actions:\n",
    "# 1) Remove numeric suffix from region (e.g., QLD1 -> QLD)\n",
    "# 2) Parse timestamp to UTC-aware datetime\n",
    "# 3) Standardize text fields (fuel_tech lowercased, facility_name stripped)\n",
    "# 4) For non-battery rows, clamp negative power_mw to 0\n",
    "# 5) (Optional) Drop duplicate (facility_code, timestamp)\n",
    "# 6) Save cleaned file and print a short summary\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------- Paths --------------------\n",
    "INPUT_CSV  = \"data/cleaned_data.csv\"              # change path if needed\n",
    "OUTPUT_CSV = \"data/cleaned_data.csv\"\n",
    "\n",
    "# -------------------- Load --------------------\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# -------------------- Region suffix removal --------------------\n",
    "# e.g., 'QLD1' -> 'QLD', 'NSW1' -> 'NSW'\n",
    "df[\"region\"] = df[\"region\"].astype(\"string\").str.replace(r\"\\d+$\", \"\", regex=True)\n",
    "\n",
    "# -------------------- Timestamp parsing --------------------\n",
    "# Convert to timezone-aware datetime (UTC). Invalid parses -> NaT\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "# Drop rows with invalid timestamps (rare, but keeps data consistent)\n",
    "df = df.dropna(subset=[\"timestamp\"])\n",
    "\n",
    "# -------------------- Text standardization --------------------\n",
    "# Keep fuel_tech normalized for robust filtering/grouping\n",
    "df[\"fuel_tech\"] = df[\"fuel_tech\"].astype(\"string\").str.lower().str.strip()\n",
    "df[\"facility_name\"] = df[\"facility_name\"].astype(\"string\").str.strip()\n",
    "\n",
    "# -------------------- Power sanity handling --------------------\n",
    "# For non-battery technologies, negative power often indicates anom./import.\n",
    "# Business rule: set negative power to 0 only for NON-battery rows.\n",
    "if \"power_mw\" in df.columns:\n",
    "    non_batt_mask = ~df[\"fuel_tech\"].fillna(\"\").str.contains(\"battery\", na=False)\n",
    "    neg_mask = df[\"power_mw\"] < 0\n",
    "    df.loc[non_batt_mask & neg_mask, \"power_mw\"] = 0\n",
    "\n",
    "# -------------------- (Optional) De-duplicate on key --------------------\n",
    "# If you want to ensure unique time points per facility, enable next line:\n",
    "# df = df.drop_duplicates(subset=[\"facility_code\", \"timestamp\"], keep=\"first\")\n",
    "\n",
    "# -------------------- Save --------------------\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "# -------------------- Summary --------------------\n",
    "print(\"✅ Cleaning complete.\")\n",
    "print(f\"Rows: {len(df):,} | Cols: {df.shape[1]}\")\n",
    "print(\"Time range:\", df[\"timestamp\"].min(), \"→\", df[\"timestamp\"].max())\n",
    "\n",
    "print(\"\\nMissing rate (top 10):\")\n",
    "print(df.isna().mean().sort_values(ascending=False).head(10))\n",
    "\n",
    "print(\"\\nRegion sample counts:\")\n",
    "print(df[\"region\"].value_counts().head(10))\n",
    "\n",
    "print(\"\\nFuel tech sample counts:\")\n",
    "print(df[\"fuel_tech\"].value_counts().head(10))\n",
    "\n",
    "print(\"\\nPreview:\")\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
